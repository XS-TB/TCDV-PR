# -*- coding: utf-8 -*-
"""PR1Tipologia_XavierSancho-Tello_LauraGandia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158wjj_ieag9fyYWcaE_obbukeqznzOMN

# Pràctica 1 Tipologia i cicle de vida de les dades: Codi Python
Xavier Sancho-Tello Bayarri i Laura Gandia Barlocci
"""

#Prèvia: instal·lació llibreries
!pip install requests
!pip install beautifulsoup4
!pip install selenium
!pip install python-whois
!pip install builtwith
!apt update #assegurar-nos que està actualitzat
!apt install chromium-chromedriver #instalem un chrome driver
!pip install selenium #instalem selenium per a gestionar el contingut dinàmic
!pip install python-dateutil

"""## Anvaluació inicial"""

# 1. Arxiu robots.txt
#https://www.wunderground.com/robots.txt
#No hi ha restriccions de permisos que afectin la nostra recerca.
import requests
from bs4 import BeautifulSoup

robots = requests.get('https://www.wunderground.com/robots.txt')
soup_robots = BeautifulSoup(robots.text)
print(robots)
print(soup_robots)

# 2. Sitemap
# https://www.wunderground.com/sitemaps/sitemap.xml

# 3. Grandària
# site: www.wunderground.com
# About 1,700,000 results (0.21 seconds)

# 4. Propietari
# El propietari del domini és TWC Product and Technology, LLC.
import whois
print(whois.whois('https://www.wunderground.com'))

# 5. Tecnologia
# Les tecnologies utilitzades "Express", "ZURB Foundation" poden utilitzar
# renders amb JavaScript. Cal doncs tenir en compte un enfocament de webscraping
# que pugui gestionar contingut dinàmic, i utilitzar navegadors headless (sense
# interfície gràfica), com Selenium.
import builtwith
print(builtwith.builtwith('https://www.wunderground.com'))

"""## Webscraping"""

# Inicialment, cal definir una funció que faci rendering per a poder gestionar
# el contingut dinàmic

import os
from bs4 import BeautifulSoup
from selenium import webdriver
import time


def rendering(url):
    # Configura las opcions del ChromeDriver
    # (degut a que treballem en entorn Google Colab)
    options = webdriver.ChromeOptions()
    options.add_argument('--headless') # deshabilitar memoria compartida
    options.add_argument('--no-sandbox') # permisos de google colab
    options.add_argument('--disable-dev-shm-usage') # opcions de chrome perquè hi ha recursos limitats
    # obrir navegador
    driver = webdriver.Chrome(options=options)
    # carregar pagina des de l'url
    driver.get(url)
    # esperar que carregui
    time.sleep(3)
    # agafar HTML source
    render = driver.page_source
    #sortir del ChromeDriver
    driver.quit()
    return render

# Definim una funció que permet extreure les dades que ens interessen
# del lloc web www.underground.com.

# Específicament, és una funció que extreu dades de la web
# https://www.wunderground.com/history/monthly/
# que conté un buscador dinàmic que permet consultar les dades meteorològiques
# històriques mensuals de tots els aeroports.

# Les dades que extreu són els valors mitjans (average) de la temperatura màxima,
# la temperatura mitjana, la temperatura mínima, el punt de rosada, la precipitació
# la profunditat de la neu, el vent, les ratxes de vent, i la pressió al nivell del mar


import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

def scrape_station_weather(station, start_date, end_date, out_directory):
    """
    Funció que extreu dades metereològiques mensuals d'una estació meteorològica
    a un determinat aeroport entre dues dates i crea un fitxer .csv amb aquestes dades.

    Concretament, realitza webscraping de https://www.wunderground.com/history/monthly/
    I Les dades que extreu són els valors mitjans (average) de la temperatura màxima,
    la temperatura mitjana, la temperatura mínima, el punt de rosada, la precipitació
    la profunditat de la neu, el vent, les ratxes de vent, i la pressió a nivell del mar.

    Parameters
    ----------
    station: str
    start_date: datetime
    start_date: datetime
    out_directory: output directori (str)

    Output
    ----------
    .csv file

    """

    # buscar la URL que després formatejarem per a trobar cada any i mes de cada estació
    search_url = 'https://www.wunderground.com/history/monthly/es/{}/LEBL/date/{}-{}'

    # crear el nom del fitxer CSV que es formatejarà segons el nom estació i any i mes
    outfile = '{}/{}_{}_{}.csv'.format(out_directory, station, start_date.year, start_date.month)

    # si el fitxer CSV no existeix, el mode serà "write"; si exiteix "append"
    if not os.path.exists(outfile):
        mode = 'w'
    else:
        mode = 'a'

    with open(outfile, mode) as f:
        # per a fitxers nous hi escrivim el nom de les columnes
        if mode == 'w':
            f.write('Date,'
                    'Max Temperature,'
                    'Avg Temperature,'
                    'Min Temperature,'
                    'Dew Point,'
                    'Precipitation,'
                    'Snowdepth,'
                    'Wind,'
                    'Gust Wind,'
                    'Sea Level Pressure\n')

        # iterar fins que arriba a la "end date"
        while start_date != end_date:

            # formatejar la URL de cerca amb els paràmetres de la funció
            format_search_url = search_url.format('K'+station,
                                                  start_date.year,
                                                  start_date.month)

            wunderground_page = rendering(format_search_url)

            wunderground_soup = BeautifulSoup(wunderground_page, 'html.parser')
            soup_container = wunderground_soup.find('lib-city-history-summary')
            soup_data = soup_container.find_all('tbody', class_='ng-star-inserted')

            row = []
            for i, dat in enumerate(soup_data):
                # iterar sobre les files
                for j, d in enumerate(dat.find_all('tr', class_='ng-star-inserted')):
                    # iterar sobre les columnes
                    for k in d.find_all('td', class_='ng-star-inserted'):
                        tmp = k.text
                        tmp = tmp.strip('  ') # eliminiar espais en blanc

                        row.append(tmp)

            # seleccionar només els valors "average" de la taula
            row_table = row[1:11:3] + row[13:19:4] + row[21:28:3]

            # incloure la data en cada fila
            f.write('{}-{},'.format(start_date.year, start_date.month))
            # escriure les dades extretes del lloc web
            f.write(','.join(row_table))
            f.write('\n') # escriure nova línia per si s'han d'afegir observacions
            start_date += relativedelta(months=1) # anar a la nova data, creant una nova URL

# connectem amb google drive
from google.colab import drive
drive.mount('/content/drive')
#%cp /content/drive/MyDrive/
#os.chdir('/content/drive/MyDrive')

# Executem la funció per l'aeroport del prat de llobregat,
# entre gener de 2000 i març de 2024 (corresponent a les dades disponibles)

station = 'el-prat-de-llobregat'
start_date = datetime(year=2000, month=1, day=1)
end_date = datetime(year=2024, month=4, day=1)

### Es pot modificar el directori ###
out_directory = '/content/drive/MyDrive/Q2/TCVD/PR1/csv'

scrape_station_weather(station, start_date, end_date, out_directory)